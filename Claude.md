# TRS - Toyota Research System

## Quick Start

**Dev Server**: http://localhost:3000 (should be running)
```bash
npm run dev
```

## Current Status

**Phase**: 2 - Agent Implementation (All Core Features Working! ‚úÖ)
**Deployed**: https://trs-mocha.vercel.app ‚úÖ
**Working**: Research ‚úÖ | Upload ‚úÖ | Browse ‚úÖ | Query Corpus ‚úÖ | PDF-only RAG ‚úÖ | URL Ingestion ‚úÖ | **Quality Classification ‚úÖ** (Session 20)
**Next**: Build Editorial Agent ‚Üí Complete 6-agent workflow
**Complete**: 5/6 agents (Research, Upload, Browse/Query, Draft ‚úÖ, Analyze ‚úÖ)
**Corpus**: 204 documents in File Search Store (Session 20), semantic RAG working, citations tested ‚úÖ
**Quality Tiers**: 4-tier classification system, auto-classifies web content, manual review for PDFs ‚úÖ (Session 20)
**File Upload**: Up to 100MB client-side, smart queue, manual metadata editing, tier selection on approval ‚úÖ
**URL Ingestion**: Jina.ai Reader ‚Üí direct text storage, duplicate detection, auto-classified as Tier 3 ‚úÖ
**Research Terms**: All 228 terms, searchable interface, bilingual toggle ‚úÖ (Session 16)
**Known Limitations**:
- ~50MB Gemini metadata extraction limit (Google API limitation)
- Files >50MB upload successfully and are fully searchable ‚úÖ
- Manual metadata entry via Edit Metadata button ‚úÖ
- 50MB warning notice displayed on Upload page ‚úÖ

## Environment Setup

```bash
cp .env.example .env.local
```

Required keys:
- `GOOGLE_AI_API_KEY` - Gemini API key
- `GOOGLE_CUSTOM_SEARCH_API_KEY` - For Research Agent
- `GOOGLE_CUSTOM_SEARCH_ENGINE_ID` - For Research Agent
- `KV_REDIS_URL` - Auto-created when connecting Vercel Redis database
- `BLOB_READ_WRITE_TOKEN` - Auto-created when connecting Vercel Blob storage

Optional features (see `.env.example`):
- `JINA_API_KEY` - Jina.ai Reader API key for URL ingestion (free: 20 req/min, with key: 500 req/min)
- `WHITELISTED_IPS` - Bypass rate limiting (comma-separated IPs, for solo dev only)

Optional security settings (see `.env.example`):
- `CLEAR_ALL_TOKEN` - Token for /api/corpus/clear-all (default: DELETE_ALL_DOCUMENTS)
- `ENABLE_MIGRATION` - Enable /api/migrate endpoint (default: false)
- `ALLOWED_ORIGIN` - CORS domain whitelist (default: http://localhost:3000)

## Security (Session 15 - Production-Ready!)

**Rate Limiting**: ‚úÖ Active (custom implementation using ioredis)
- `/api/summary`: 10/hour, 2/min burst
- `/api/search`: 20/hour, 3/min burst
- `/api/process-blob`: 15/hour, 3/min burst
- `/api/process-url`: 15/hour, 3/min burst
- Returns HTTP 429 with retry-after when exceeded
- **IP Whitelist**: Optional bypass for solo dev (WHITELISTED_IPS env var)

**Input Validation**: ‚úÖ Active
- Prompt injection detection (15+ patterns blocked)
- Length limits: 1000 chars (query), 500 chars (custom instructions), 50 messages (history)
- SSRF protection (blob URL domain validation)
- Path traversal protection (filename sanitization)

**Protected Endpoints**: ‚úÖ
- `/api/corpus/clear-all` - Requires confirmation token
- `/api/migrate` - Requires ENABLE_MIGRATION=true flag

**Security Headers**: ‚úÖ Active (in `next.config.ts`)
- X-Frame-Options, X-Content-Type-Options, HSTS, CSP, Permissions-Policy
- CORS configured (whitelist via ALLOWED_ORIGIN)

**Testing**:
```bash
# Clear rate limits before testing
node test/clear-rate-limits.js

# Run security test suite
bash test/security-test.sh
```

## Project Structure

```
app/
  api/              ‚Üê API routes for all agents + utilities
    process-blob/   ‚Üê Upload processing (Blob ‚Üí File Search Store + metadata)
    process-url/    ‚Üê URL ingestion via Jina.ai Reader (Session 16)
    summary/        ‚Üê Query Corpus with semantic RAG (File Search tool)
    web-search/     ‚Üê Google Search for generic web knowledge (Session 21)
    migrate/        ‚Üê One-time migration endpoint (protected with env flag)
    corpus/
      clear-all/    ‚Üê Delete all documents (protected with token)
      delete/       ‚Üê Delete single document (rate limited)
  page.tsx          ‚Üê Main UI with 7 tabs (Research, Upload, Browse, Search Web, Draft, Analyze, Editor)
components/
  agents/           ‚Üê 6 agents complete (Research, Upload, Browse/Query, Web Search, Draft, Analyze)
    browse-query-agent.tsx ‚Üê Query Corpus (simplified, no Mode/Length/Custom)
    web-search-agent.tsx   ‚Üê Search Web with Google Search (Session 21)
  ui/
    term-browser.tsx ‚Üê Searchable term browser with bilingual toggle (Session 16)
lib/
  gemini.ts         ‚Üê Gemini 2.5 Flash client
  file-search-store.ts ‚Üê File Search Store (semantic RAG) - PRIMARY STORAGE
  file-search.ts    ‚Üê Files API (deprecated, only for temp metadata extraction)
  blob-storage.ts   ‚Üê Vercel Blob (permanent file storage for downloads)
  kv.ts             ‚Üê Vercel KV (Redis) for metadata + status tracking + rate limiting
  rate-limit.ts     ‚Üê Custom rate limiter (sliding window, ioredis)
  sanitize.ts       ‚Üê Input validation & prompt injection protection
  research-terms-data.ts ‚Üê 228 research terms with 4-level hierarchy (Session 16)
  types.ts          ‚Üê TypeScript definitions
test/
  security-test.sh  ‚Üê Security test suite (rate limiting + injection tests)
  clear-rate-limits.js ‚Üê Utility to reset rate limits
```

## Tech Stack

- Next.js 16 + TypeScript
- Tailwind CSS v3 + Shadcn/ui (NOTE: v4 causes build issues - stay on v3)
- Google Gemini 2.5 Flash (PDF reading, metadata extraction, RAG queries)
- **Google File Search Store** - Persistent semantic RAG (automatic chunking/embeddings)
- Vercel Blob for permanent file storage (downloads/display)
- ioredis + Vercel KV (Redis) for metadata + approval workflow
- Google Custom Search API for web search
- react-dropzone for file uploads
- `@google/genai` SDK v1.29.0 (official SDK for File Search Store)
- **Jina.ai Reader** - Web page content extraction for URL ingestion (Session 16, simplified Session 17)

## ‚ö†Ô∏è CRITICAL: Gemini Model Policy

**ALWAYS USE: `gemini-2.5-flash`** (current stable model)

**NEVER USE:**
- ‚ùå `gemini-1.5-*` (deprecated, inferior performance)
- ‚ùå `gemini-2.0-*` (deprecated, being phased out)
- ‚ùå `gemini-2.2-*` (does not exist)
- ‚ùå Any `-exp` experimental models (unstable)

**Why this matters:**
Claude's training data is dated. When encountering errors, Claude may incorrectly assume older models (1.5, 2.0) are "more stable" - this is almost never the case. Google deprecates older models because they don't work as well.

**Migration path:**
- Current: `gemini-2.5-flash` (stable, production-ready)
- Future: `gemini-3.0-flash` (when available via API key in ~2 weeks)

**If you see model errors:**
1. Check model name is exactly `gemini-2.5-flash`
2. Do NOT downgrade to 1.5 or 2.0
3. Check Google AI Studio for current model availability

## RAG Implementation (Session 12 - FIXED)

**Uses Google File Search Store** - Persistent semantic RAG with automatic chunking, embeddings, and retrieval. Handles 100+ documents without token limit issues.

**Three-Layer Architecture:**
1. **Vercel Blob** - Original files (PDFs + text from URLs, permanent, for downloads)
2. **File Search Store** - Chunked + embedded text (permanent, semantic search)
3. **Redis (KV)** - Metadata + approval status + references to layers 1 & 2

**Data Flow:**
```
Upload ‚Üí Blob (permanent) ‚Üí File Search Store (chunked/embedded) ‚Üí Metadata in Redis
Query ‚Üí Redis (which docs approved?) ‚Üí File Search tool (semantic search) ‚Üí Gemini (answer + citations)
Download ‚Üí Redis (get blobUrl) ‚Üí Fetch from Blob ‚Üí Browser
```

**Key Technical Details:**
- Store ID: `fileSearchStores/toyotaresearchsystem-b8v65yx9esml`
- Chunking: 500 tokens/chunk, 50 token overlap
- Semantic retrieval: Returns top 5-10 chunks (~2,500 tokens) instead of all documents
- **99.77% token reduction** (1M+ tokens ‚Üí 2,500 tokens)
- Scales to 1000+ documents

**Citation System:**
- Extracts from grounding metadata (`groundingChunks` array)
- Matches chunk titles to original documents by filename
- Parses page numbers from chunk text (`--- PAGE X ---` markers)
- Format: `[CitationKey, p.5]` or `[FamilyName2024, p.1, 2, 5]`

## 7 Active Agents (1 Eliminated)

1. **Research** ‚úÖ COMPLETE - 228 curated terms with searchable browser, bilingual toggle, Google Custom Search, targeted search (J-STAGE, Patents, Scholar) - REDESIGNED Session 16
2. **Upload** ‚úÖ COMPLETE - Client-side Blob upload, smart queue, up to 100MB, ~50MB metadata limit, URL ingestion via Jina.ai - URL INGESTION ADDED Session 16
3. **Browse** ‚úÖ COMPLETE - Sorting, infinite scroll, file details modal, delete functionality, quality tier management
4. **Query Corpus** ‚úÖ COMPLETE (Session 21 - Simplified) - Semantic RAG with strict citations, no UI constraints
5. **Search Web** ‚úÖ COMPLETE (Session 21) - Google Search for generic web knowledge, separate from corpus
6. **Draft** ‚úÖ COMPLETE (Session 18) - Outline ‚Üí full article generation with corpus grounding
7. **Analyze** ‚úÖ COMPLETE (Session 19 - Redesigned) - TRS database validation, surfaces alternatives worth considering
8. **Editorial** üî® OPTIONAL - Final polish for structure, grammar, and clarity (external LLMs may be sufficient)

**Note:** Images eliminated from scope (user decision: PDF-only corpus)

**Architecture Decision (Session 21)**: Separated Query Corpus and Search Web into independent tabs to highlight knowledge gap between authoritative corpus facts and generic web knowledge.

## Upload Agent Architecture (Session 10 - Production-Ready!)

**Client-Side Blob Upload** - Bypasses 4.5MB serverless function limit:
- Browser uploads directly to Vercel Blob (up to 100MB)
- Server processes from Blob URL (no body size limit!)
- Two-phase progress: Upload (10-50%) ‚Üí Processing (50-100%)
- Multipart upload for files >5MB

**Smart Queue System** (Êú™ÁÑ∂Èò≤Ê≠¢ - mizen boushi):
- Size-based concurrency: Small files (<10MB): 5 concurrent, Medium (10-30MB): 3 concurrent, Large (>30MB): 2 concurrent
- Bulk upload warning: 3+ large files or >100MB total triggers confirmation dialog
- Queue status card: Shows "Processing: 2" and "Queued: 5" with dynamic limits
- Prevents browser crashes and API rate limits

**Pending Review Persistence**:
- Files survive navigation, page refreshes, browser sessions
- Badge counter on Upload tab shows pending count (refreshes every 10s)
- Load pending files from Redis on mount

**UX Polish**:
- Processing Queue shows only active files (queued/processing)
- Completed files move to Review Dashboard
- File sizes displayed (e.g., "45.2MB")
- Blue pulsing dot for active uploads
- Memory optimization (clears raw files after upload)

**Tested & Validated**:
- ‚úÖ Files up to 100MB upload to Blob successfully
- ‚úÖ Bulk uploads (10+ files) queue properly
- ‚úÖ Vercel Pro timeout (120s) handles processing
- ‚úÖ No HTTP 413 errors, no crashes, smooth UX

**URL Ingestion (Session 16-17)**:
- Jina.ai Reader API integration for clean content extraction
- Stores as text files (.txt) - no PDF conversion needed (Session 17 simplification)
- Duplicate URL detection via Redis source tracking
- Queue system with same status display
- Same review workflow and approval process
- Source URL displayed in Browse tab for easy duplicate checking
- Perfect for ingesting 75+ Toyota history web pages

## ‚úÖ Session 12 - Critical Architectural Fix

### Problem Discovered (End of Session 11)
- **Error**: Token limit exceeded (1,048,576 tokens) with 36+ documents
- **Root Cause**: Used Files API (no semantic retrieval) instead of File Search Store
- **Impact**: System sent ALL documents to Gemini on every query = crash

### Solution Implemented (Session 12)
- **Migrated to File Search Store** with automatic semantic retrieval
- **Token reduction**: 1M+ tokens ‚Üí ~2,500 tokens (99.77% reduction)
- **Migration results**: 30/36 documents migrated successfully
  - 6 failed due to Japanese characters in filenames (user will re-upload)
- **New architecture**: Blob ‚Üí File Search Store ‚Üí Redis (3-layer storage)
- **Citation extraction**: Rewrote to parse grounding metadata from File Search Store

### Technical Changes
- Created `lib/file-search-store.ts` - File Search Store integration
- Updated `app/api/process-blob/route.ts` - Upload to File Search Store
- Updated `app/api/summary/route.ts` - Query with File Search tool
- Created `app/api/migrate/route.ts` - One-time migration endpoint

### Status: ‚úÖ FIXED
- Query Corpus works with 30 documents (no token errors)
- Semantic retrieval working (returns only relevant chunks)
- Citations fixed (pending user test after deployment)
- Scales to 100+ documents (tested to 1000+)

**See**: `docs/progress/2025-11-14-Session12.md` for detailed implementation

---

## ‚úÖ Session 14 - Query Customization & Research Organization

### Query Corpus Customization (Session 14)
**Added 3-dimensional control over RAG responses:**

1. **Mode** - What to focus on:
   - Standard, Find Examples, Find People, Compare Approaches, Timeline/History, Technical Deep-Dive

2. **Length** - How much detail:
   - Brief (2-3 sentences), Medium (2-3 paragraphs), Detailed (4-6 paragraphs)

3. **Custom Instructions** - User-specific context (collapsible textarea)

**Example:** Mode: "Find Examples" + Length: "Detailed" + Custom: "Focus on 1990s Japanese implementations"

**UI Change:** "Send" button ‚Üí "Search" button

### Research Agent 2-Level Navigation (Session 14)
**Hierarchical dropdown navigation:**
- **Step 1:** Select Track (PD, PE, TPS, Cross-Cutting)
- **Step 2:** Select Subcategory (dynamically updates, optional "All")
- **Step 3:** Select Terms (filtered by subcategory)

**Example:** Track: "PE" ‚Üí Subcategory: "Tooling Engineering" ‚Üí Terms: 6 sub-areas

### Research Terms Reorganization (Session 14)
**Hierarchical structure with logical subcategories:**

- **Track 1: PD** (4 subcategories)
  - Design & Development Process, CAD/PLM Systems, Simulation & Virtual Validation, Prototyping & Testing

- **Track 2: PE** (5 subcategories)
  - Production Preparation, Process Design, **Tooling Engineering (6 sub-areas)**, Manufacturing Processes, Supplier Collaboration

- **Track 3: TPS** (6 subcategories)
  - TPS Core, Kaizen & Methods Analysis, Quality Control, Daily Ops & SMED, Automation & Measurement, **3 Pillar Activity System (new)**

**Key Additions:**
- Tooling Engineering expanded under PE (cutting tools, jigs, tool management)
- 3 Pillar Activity System (3Êú¨Êü±Ê¥ªÂãï) from 2007 Kamigo plant framework
- Replaced Engineering Kaizen with Methods Analysis

**See**: `docs/progress/2025-11-14-Session14.md` for detailed implementation

---

## ‚úÖ Session 16 - Research Agent Redesign + URL Ingestion

### Problem Discovered
- **Research Agent Issues**: Dropdown structure didn't match research_terms.md
  - Categories appeared "hallucinated"
  - Missing ~50+ terms (only ~170 of 228 included)
  - PE Tooling Engineering reduced to 7 terms (should be 36 across 6 sub-areas)
  - Missing entire "3 Pillar Activity System" subcategory
  - No bilingual search capability

### Solution 1: Research Agent Redesign

**1. Rebuilt Data Structure** (`lib/research-terms-data.ts`)
- Parsed `research_terms.md` exactly as documented
- All 228 curated terms included
- Proper 4-level hierarchy: Track ‚Üí Subcategory ‚Üí Sub-area ‚Üí Terms
- Fixed structure:
  - **PD**: 4 subcategories (was 5 artificial splits)
  - **PE**: 5 subcategories (was 8 over-split)
    - Tooling Engineering: 6 sub-areas, 36 terms ‚úÖ
  - **TPS**: 6 subcategories (added missing 3 Pillar Activity)
  - **Cross-Cutting**: 2 subcategories (unchanged)

**2. New Term Browser** (`components/ui/term-browser.tsx`)
- Replaced 3-dropdown system with searchable list
- Two-column layout:
  - Left: Collapsible category filters with term counts
  - Right: Scrollable term list with checkboxes
- **Language toggle**: English Only | ‰∏≠,Êó• Only | Both
- Real-time search across all terms
- Selected terms badge display
- Context path shown (Track ‚Üí Subcategory ‚Üí Sub-area)

**3. Updated Research Agent** (`components/agents/research-agent.tsx`)
- Removed dropdown logic
- Integrated TermBrowser component
- Simplified state management
- All existing functionality preserved

### Solution 2: URL Ingestion System

**User Request**: Wanted to ingest 75+ Toyota history web pages without manual "Save as PDF" for each.

**1. Backend API** (`app/api/process-url/route.ts`)
- **Jina.ai Reader**: Calls `https://r.jina.ai/{url}` for clean content extraction
- **Smart Extraction**: Removes navigation, ads, headers
- **Text Storage**: Stores as `.txt` file (Session 17 - simplified from PDF conversion)
- **Blob Upload**: Stores text file in Vercel Blob
- **File Search Store**: Adds to semantic RAG
- **Metadata Extraction**: Gemini extracts title, summary, keywords, track (including History track)
- **Duplicate Detection**: Checks Redis, returns 409 with existing document info
- **Source Tracking**: Stores original URL in `metadata.source`

**2. UI Integration** (`components/agents/upload-agent.tsx`)
- Added URL section below file upload (purple card)
- URL input with "Add to Queue" button
- Queue display with status icons:
  - ‚è± Pending
  - ‚ü≥ Processing (spinning)
  - ‚úÖ Complete (green)
  - ‚ùå Error (red)
  - ‚ö†Ô∏è Duplicate (amber)
- "Process All" button for batch processing
- Same Pending Review dashboard integration

**3. Dependencies**
- None required (Session 17 - removed PDF conversion dependencies)

### Workflow
1. Paste URL in Upload tab
2. Add to queue (can add multiple)
3. Click "Process All"
4. System:
   - Fetches via Jina.ai
   - Converts to PDF
   - Uploads to Blob
   - Processes through File Search Store
   - Extracts metadata
   - Stores in Redis with source URL
5. Appears in Pending Review
6. Approve ‚Üí Corpus

### Results
- ‚úÖ 228 complete terms with accurate hierarchy
- ‚úÖ Bilingual search capability
- ‚úÖ Automated web page ingestion
- ‚úÖ Duplicate detection
- ‚úÖ Queue system for batches
- ‚úÖ Integrated workflow
- ‚úÖ Perfect for Toyota history (75+ pages)

**New API Endpoint:**
```
POST /api/process-url
Body: { url: string }
Returns: Same format as /api/process-blob
Errors:
  400 - Invalid URL
  409 - Duplicate (with existing document details)
  429 - Rate limit exceeded
  500 - Processing failed
```

**Known Limitations:**
- **Jina.ai Rate Limits**: Free tier = 20 req/min (tracked by IP). Optional `JINA_API_KEY` increases to 500 req/min.
- **Text-only storage**: Web content stored as text files, not PDFs (Session 17)
- **No Authentication**: Can't ingest pages behind login
- **Rate Limiting**: 15/hour, 3/min burst (TRS endpoint limit, can bypass with IP whitelist for solo dev)

**See**: `docs/progress/2025-11-14-Session16.md` for detailed implementation

---

## ‚úÖ Session 19 - Analyze Agent Redesign (TRS Database Validation)

### Problem Discovered (Session 18)
- **Massive output**: Thousands of words, line-by-line analysis (unreadable)
- **Too prescriptive**: "Use better examples" instead of "here's what TRS database shows"
- **Corpus size issue**: 165 documents caused empty responses (bloated prompt)

### Solution Implemented (Session 19)
- **UI Simplified**: Drag/drop only (removed paste textarea)
- **Analysis Reframed**: "TRS Database Validation" not "Writing Coach"
  - Analytical tone: "TRS also contains..." not "You should..."
  - Surfaces alternatives: "Database shows X, Y, Z worth considering..."
  - Focus: What's IN the TRS corpus, not general writing advice
- **Scalability Fixed**: Removed corpus context from prompt (File Search tool already has access)
  - Now handles 165+ documents without issue
  - Previous: 10,000+ chars of metadata = context overflow
  - After: Minimal reference, File Search does the work

### Results
- ‚úÖ Concise output (~500 words vs. thousands)
- ‚úÖ Surfaces alternatives from TRS database
- ‚úÖ Analytical, not prescriptive
- ‚úÖ Scales to 165+ documents (tested successfully)
- ‚úÖ User feedback: "I think it is good enough"

**See**: `docs/progress/2025-11-15-Session19.md` for detailed implementation

---

## ‚úÖ Session 20 - Document Quality Classification System

### Problem Discovered
- **Corpus growth diluting quality**: 200+ documents now includes high-quality primary sources mixed with timelines and general history
- Generic content (timelines, web articles) crowding out authoritative sources in RAG responses
- No way to prioritize ex-Toyota expert sources over background material
- User insight: All `.txt` files are URL-ingested web content ‚Üí can auto-classify as Tier 3

### Solution Implemented
**4-Tier Quality Classification System** with conservative auto-classification + manual review UI

**1. Classification Schema** (`lib/types.ts`)
- Added `qualityTier`, `tierLabel`, `autoClassified`, `classifiedAt` fields to DocumentMetadata
- 4 tiers: Tier 1 (Authoritative), Tier 2 (High Quality), Tier 3 (Supporting), Tier 4 (Background)

**2. Conservative Auto-Classification** (`lib/classify-documents.ts`)
- Tier 4: History track + timeline keywords
- Tier 3: URL-ingested text files (`.txt` with source URL) - all web content
- Tier 2: Academic papers and default for PDFs (safe, requires manual review)
- Tier 1: None (user manually promotes best sources)

**3. Quality Review UI** (Browse ‚Üí Quality Review tab)
- Collapsible tier sections with document counts
- Inline tier dropdown to change classifications
- Shows auto-classification reason for transparency
- Batch save functionality
- Filter/search/sort options
- Stats bar shows tier distribution: Tier 1 (0) | Tier 2 (38 PDFs) | Tier 3 (146 .txt) | Tier 4 (20)

**4. Upload Integration**
- Tier selection dropdown in Pending Review (defaults to Tier 2)
- Shows tier descriptions
- Mandatory selection before approval

**5. Agent Prompt Updates**
- **Query Corpus**: Prioritizes Tier 1+2 sources, uses Tier 3 for context, Tier 4 only for dates
- **Draft Agent**: Outline and full draft generation prioritize high-tier sources
- **Analyze Agent**: Surfaces Tier 1+2 alternatives when article uses lower-tier sources

### Results
- ‚úÖ 204 documents classified: 38 PDFs (Tier 2), 146 .txt (Tier 3), 20 timelines (Tier 4)
- ‚úÖ User only needs to review 38 PDFs to identify Tier 1 sources (vs. 204 documents)
- ‚úÖ Automatic prioritization in all agents - no prompting required
- ‚úÖ Quality improvement: RAG queries now prioritize authoritative sources over timelines

**See**: `docs/progress/2025-11-15-Session20.md` for detailed implementation

---

## ‚úÖ Session 21 - Two-Tab Architecture: Query Corpus + Search Web (2025-01-18)

### Bug Fixes Completed
- ‚úÖ **Reject Button Fixed**: Now properly deletes from File Search Store (not just Redis)
- ‚úÖ **Follow-up Queries Fixed**: Removed 1000-char limit on history messages

### Architectural Pivot: Sequential ‚Üí Separate Tabs
**Problem**: Sequential dual-query executed but Query 2 returned generic "Lean 101" knowledge while corpus had specific facts

**User Insight**: "The google search of the internet will always return the perceived common answer but not the best facts and details."

**Solution**: Separated into two independent tabs to highlight knowledge gap
- **Query Corpus Tab**: Strict citations, corpus-only responses, specific dates/names/tools
- **Search Web Tab**: Google Search, generic web knowledge, surface-level principles

### Implementation Changes
**Backend**:
- Reverted `/api/summary` to corpus-only with strict citation requirements
- Created new `/api/web-search` endpoint (Google Search tool)
- Added anti-duplication instruction ("CRITICAL: Provide ONE clear, concise response")
- Removed Mode/Length/Custom Instructions parameters

**Frontend**:
- Simplified `browse-query-agent.tsx` (removed Mode/Length/Custom UI)
- Created `web-search-agent.tsx` (clean query input only)
- Updated `app/page.tsx` (7 tabs: added "Search Web")

**Citation Requirements (Corpus)**:
- "You MUST ONLY use information retrieved by the File Search tool"
- "You MUST cite EVERY factual claim using [CitationKey, p.#]"
- "Every sentence with factual information MUST include citation"

### Results
**Corpus Query**: Specific dates (1934, 1950s), names (Kiichiro Toyoda), tools (TOGO CAD, Shusa system), full citations like `[History2012, p.1]`

**Web Search**: Generic principles (Chief Engineer, Set-Based Design), no specific implementations, abstract concepts

**Impact**: System now demonstrates hidden knowledge gap between authoritative TRS corpus and generic web knowledge

**See**: `docs/progress/2025-01-18-Session21.md` for detailed implementation

---

## ‚úÖ Session 17 - URL Ingestion Simplification + History Track

### Architecture Simplification
**Problem**: PDF conversion was unnecessary complexity
- 60MB+ dependencies (@sparticuz/chromium, puppeteer-core, md-to-pdf)
- Slower processing (Puppeteer startup overhead)
- File Search Store accepts text/plain natively

**Solution**: Store markdown as text files directly
- Changed `app/api/process-url/route.ts` to create `.txt` files
- Removed all PDF conversion dependencies
- Faster, simpler, smaller deployment

### History Track Addition
**Need**: Category for Toyota historical/biographical content
- Added "History" to ResearchTrack enum
- Updated metadata extraction with History examples
- Gemini now classifies founder biographies, timelines, company milestones

### IP Whitelist for Solo Dev
**Enhancement**: Bypass rate limiting during development
- Added `WHITELISTED_IPS` env var to `lib/rate-limit.ts`
- Current IP: `99.137.185.29`
- Speeds up solo dev workflow
- Production safety: Empty whitelist = full rate limiting

### UI Enhancement
**Browse tab now shows source URLs**
- Easy duplicate checking for URL-ingested documents
- Click to open original page
- Shows "Source: [URL]" in file details modal

### Testing Results
- ‚úÖ Toyota history page ingested successfully
- ‚úÖ Track classified as "History"
- ‚úÖ Excellent content quality from Jina.ai
- ‚úÖ Fully searchable in corpus
- ‚úÖ 60MB smaller deployment

**See**: `docs/progress/2025-11-14-Session17.md` for detailed implementation

---

## Known Issues (Current)

### ‚ö†Ô∏è Known Limitations

**2. ~50MB Gemini Metadata Extraction Limit**
- Issue: Gemini API has undocumented ~50-52MB limit for PDF processing
- Source: Known Google API limitation (contradicts 2GB documented limit)
- Impact: Files >50MB upload successfully but metadata extraction fails
- Behavior:
  - ‚úÖ File uploads to Blob
  - ‚úÖ File uploads to File Search (fully indexed and queryable!)
  - ‚ùå Metadata extraction fails
  - ‚ö†Ô∏è Shows "Metadata extraction failed. Please review manually."
- **IMPORTANT**: Files are still fully queryable in RAG - only metadata display is affected
- **Workaround**: Manually enter metadata using Edit Metadata button ‚úÖ (Session 13 - FIXED!)
- **Warning displayed**: Upload page shows 50MB notice with workaround instructions ‚úÖ
- Alternative solutions:
  - Compress PDFs in Adobe Acrobat (can reduce 50-80% for scanned docs)
  - Skip metadata extraction for 50MB+ files, mark for manual review
  - Add bulk metadata editor in Browse tab

## Key Design Decisions

- No authentication (single user on desktop)
- AI-assisted metadata with human review workflow
- Gemini 2.5 Flash for all operations (upgrade to 3.0 when available)
- **Three-layer storage architecture** (Session 12):
  - Vercel Blob: Original PDFs for downloads
  - File Search Store: Chunked + embedded text for semantic RAG
  - Redis: Metadata + approval status + references
- **Semantic RAG with File Search Store**: Managed service handles chunking, embeddings, retrieval
  - Tradeoff: Less transparency, more simplicity
  - Future: Can migrate to Pinecone/Supabase pgvector if more control needed
- **Client-side Blob upload** (Session 10): Bypasses 4.5MB limit, supports up to 100MB
- **PDF-only corpus** (User decision): Eliminates image complexity, focuses on documents
- Session-based conversations (no persistence)

## Documentation

- `TRS_Concept.md` - Original specification
- `Next_steps.md` - Current work queue and priorities
- `research_terms.md` - Master list of Japanese/English search terms for Research Agent
- `docs/progress/` - Session logs by date (completed work)
- `docs/implementation/` - **Implementation guides for complex features** (step-by-step plans)
  - `client-side-blob-upload.md` - Fix large file upload (HTTP 413)
- `README.md` - Full project documentation

## Common Commands

```bash
npm run dev          # Start dev server
npm run build        # Production build
npm run lint         # ESLint check
```

## Getting Context Quickly

Read this file + `Next_steps.md` + latest progress log in `docs/progress/`
